<!DOCTYPE html><html>
 <head>
  <meta charset="utf-8"/>
  <title>
   Writing an LRU cache for a new language
  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="apple-mobile-web-app-capable" content="yes"/>
  <link href="http://fonts.googleapis.com/css?family=Raleway:400,100,200,300,500,600,700,800,900" rel="stylesheet" type="text/css"/>
  <link href="lib/skeleton/css/skeleton.css" rel="stylesheet"/>
  <link href="lib/highlightjs/styles/default.css" rel="stylesheet"/>
  <link href="css/style.css" rel="stylesheet"/>
  <link href="blog.css" rel="stylesheet"/>
  <script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-63286859-1', 'auto');
          ga('send', 'pageview');

  </script>
 </head>
 <body>
  <div class="container wallapatta-container">
   <div class="header">
    <h1>
     <a href="index.html">
      CHETHIYA ABEYSINGHE
     </a>
    </h1>
    <a class="button" href="https://www.twitter.com/chethiyaa">
     @chethiyaa
    </a>
   </div>
   <div class="wallapatta">
    <h1 class="title">
     Writing an LRU cache for a new language
    </h1>
    <h3 class="date">
     Nov 05, 2019
    </h3>
    <div class="row">
     <div class="wallapatta-main nine columns">
      <div id="wallapatta_0" class="article"><div id="wallapatta_1" class="section"><h3 class="heading"><span id="wallapatta_2" class="block"><span id="wallapatta_186" class="text">LRU Cache</span></span></h3><div class="content"></div></div><div id="wallapatta_3" class="section"><div class="content"><p id="wallapatta_4" class="paragraph"><span id="wallapatta_187" class="text">Caching is a quite useful concept, specially if you have relatively heavy computations that might not need to be done every single time. Among many different types of caches, LRU caches are a quite popular choice. The basic idea is to retain most recently used </span><em id="wallapatta_188" class="italics"><span id="wallapatta_189" class="text">n</span></em><span id="wallapatta_190" class="text"> outcomes that can fit inside the allocated memory for the cache. So those cached outcomes can be used to serve subsequent queries without recomputing those outcomes. This is quite a good fit for most practical use cases when:</span></p></div></div><ul id="wallapatta_8" class="list"><li id="wallapatta_9" class="list-item"><span id="wallapatta_10" class="block"><span id="wallapatta_195" class="text">same query is likely to occur in a reasonably small time window,</span></span></li><li id="wallapatta_11" class="list-item"><span id="wallapatta_12" class="block"><span id="wallapatta_196" class="text">computed outcomes for queries are time sensitive. i.e. outcomes depends on the time it's computed.</span></span></li></ul><div id="wallapatta_16" class="section"><div class="content"><p id="wallapatta_17" class="paragraph"><span id="wallapatta_198" class="text">I was recently asked to implement an LRU cache during a coding interview and I was quite happy that interviewers do ask non hypothetical questions too. Anyway this article is not about interviews. It's an LRU cache implementation for a new programming language called </span><a id="wallapatta_199" class="link" href="https://github.com/ballerina-platform/ballerina-lang">Ballerina</a><span id="wallapatta_200" class="text">.</span></p></div></div><div id="wallapatta_21" class="section"><h3 class="heading"><span id="wallapatta_22" class="block"><span id="wallapatta_206" class="text">Current cache implementation of Ballerina in its Standard Library</span></span></h3><div class="content"></div></div><div id="wallapatta_23" class="section"><div class="content"><p id="wallapatta_24" class="paragraph"><span id="wallapatta_207" class="text">I wanted to give Ballerina a try for a long time, but I didn't get a chance. Finally I got some time to dig into their codebase and try it out. I was quite impressed by the fact that they have included an </span><a id="wallapatta_208" class="link" href="https://ballerina.io/learn/api-docs/ballerina/cache/index.html">LRU cache implementation in their standard library</a><span id="wallapatta_209" class="text"> and it's shipped together with the compiler. It looks like that caching implementation is consumed in few other places inside other standard libraries, like the </span><a id="wallapatta_210" class="link" href="https://ballerina.io/learn/api-docs/ballerina/http/index.html">HTTP module</a><span id="wallapatta_211" class="text">.</span></p></div></div><div id="wallapatta_25" class="section"><div class="content"><p id="wallapatta_26" class="paragraph"><span id="wallapatta_212" class="text">I looked at their caching implementation and found these issues with it:</span></p></div></div><ul id="wallapatta_27" class="list"><li id="wallapatta_28" class="list-item"><span id="wallapatta_29" class="block"><span id="wallapatta_213" class="text">Its runtime complexity is not the optimal for a LRU Cache. If size of the buffer is </span><code id="wallapatta_214" class="code"><span id="wallapatta_215" class="text">N</span></code><span id="wallapatta_216" class="text"> and if there are </span><code id="wallapatta_217" class="code"><span id="wallapatta_218" class="text">M</span></code><span id="wallapatta_219" class="text"> cache queries (assuming GET/SET ratio is constant) then its run time complexity is </span><code id="wallapatta_220" class="code"><span id="wallapatta_221" class="text">O(NM)</span></code><span id="wallapatta_222" class="text">. The LRU Cache can be implemented to run in </span><code id="wallapatta_223" class="code"><span id="wallapatta_224" class="text">O(M)</span></code><span id="wallapatta_225" class="text">.</span></span></li></ul><ul id="wallapatta_33" class="list"><li id="wallapatta_34" class="list-item"><span id="wallapatta_35" class="block"><span id="wallapatta_227" class="text">It has a parameter called </span><em id="wallapatta_228" class="italics"><span id="wallapatta_229" class="text">eviction factor</span></em><span id="wallapatta_230" class="text"> </span><code id="wallapatta_231" class="code"><span id="wallapatta_232" class="text">f</span></code><span id="wallapatta_233" class="text">, and what it does is once the cache is full it removes </span><code id="wallapatta_234" class="code"><span id="wallapatta_235" class="text">N*f</span></code><span id="wallapatta_236" class="text"> least recently used items. This behavior doesn't utilize allocated memory for the cache in 100%, but average utilization is something like </span><code id="wallapatta_237" class="code"><span id="wallapatta_238" class="text">(1-f)*100 + (f*100/2)</span></code><span id="wallapatta_239" class="text"> percent.</span></span></li><li id="wallapatta_36" class="list-item"><span id="wallapatta_37" class="block"><span id="wallapatta_240" class="text">It compares between all timestamps of the items to compute the least recently used </span><code id="wallapatta_241" class="code"><span id="wallapatta_242" class="text">N*f</span></code><span id="wallapatta_243" class="text"> items to be evicted. But this depends on the precision of the timestamp. It currently stores timestamps in milliseconds. So if all items in the cache are accessed in the same millisecond, then above computation would be inaccurate.</span></span></li><li id="wallapatta_38" class="list-item"><span id="wallapatta_39" class="block"><span id="wallapatta_244" class="text">It has a separate task running every 5 seconds to cleanup expired cache entries. The way it's implemented this also introduces a shared lock between all cache instances. Altogether this cleanup processes is unnecessary work as this check can be done only during the GET operation. Also it doesn't make much sense to do this regular cleanup work for saving memory. Usually LRU caches are expected to be full all the time. If not consumer should consider reducing the amount of memory (i.e. </span><code id="wallapatta_245" class="code"><span id="wallapatta_246" class="text">N</span></code><span id="wallapatta_247" class="text">) allocated for the cache.</span></span></li><li id="wallapatta_40" class="list-item"><span id="wallapatta_41" class="block"><span id="wallapatta_248" class="text">It seems to be written in a non thread-safe manner. See </span><a id="wallapatta_249" class="link" href="https://github.com/ballerina-platform/ballerina-lang/issues/19187#issuecomment-544975764">here</a><span id="wallapatta_250" class="text"> the comment I made in an issue.</span></span></li></ul><div id="wallapatta_42" class="special"><div id="wallapatta_43" class="section"><div class="content"><p id="wallapatta_44" class="paragraph"><strong id="wallapatta_251" class="bold"><span id="wallapatta_252" class="text">Here how the current implementation takes </span><code id="wallapatta_253" class="code"><span id="wallapatta_254" class="text">O(NM)</span></code><span id="wallapatta_255" class="text"> time:</span></strong></p></div></div><div id="wallapatta_45" class="section"><div class="content"><p id="wallapatta_46" class="paragraph"><span id="wallapatta_256" class="text">Once cache is reached to it's capacity it computes </span><code id="wallapatta_257" class="code"><span id="wallapatta_258" class="text">Nf</span></code><span id="wallapatta_259" class="text"> least recently used entries in cache. That's done in </span><code id="wallapatta_260" class="code"><span id="wallapatta_261" class="text">O(N*Nf)</span></code><span id="wallapatta_262" class="text">, using 2 loops. This could have been done much faster using a heap, but that approach won't help us much to get the overall time complexity down to </span><code id="wallapatta_263" class="code"><span id="wallapatta_264" class="text">O(M)</span></code></p></div></div><div id="wallapatta_47" class="section"><div class="content"><p id="wallapatta_48" class="paragraph"><span id="wallapatta_265" class="text">Out of </span><code id="wallapatta_266" class="code"><span id="wallapatta_267" class="text">M</span></code><span id="wallapatta_268" class="text"> cache access, if we assume the ratio between GET/SET operations is constant, then we can assume the above eviction is going to happen </span><code id="wallapatta_269" class="code"><span id="wallapatta_270" class="text">O(M/(N*f))</span></code><span id="wallapatta_271" class="text"> times. Note that the "Big O" eliminates that constant factor.</span></p></div></div><div id="wallapatta_49" class="section"><div class="content"><p id="wallapatta_50" class="paragraph"><span id="wallapatta_272" class="text">So total run time complexity is </span><code id="wallapatta_273" class="code"><span id="wallapatta_274" class="text">O((M/Nf) * (N*Nf))=O(MN)</span></code><span id="wallapatta_275" class="text">.</span></p></div></div></div><div id="wallapatta_54" class="section"><h3 class="heading"><span id="wallapatta_55" class="block"><span id="wallapatta_277" class="text">How to do it in </span><code id="wallapatta_278" class="code"><span id="wallapatta_279" class="text">O(M)</span></code></span></h3><div class="content"></div></div><div id="wallapatta_56" class="section"><div class="content"><p id="wallapatta_57" class="paragraph"><span id="wallapatta_280" class="text">We should be able to implement LRU cache so that its time complexity is independent from the cache size. Key idea to do so is to use a separate linked list to store entries sorted by their access time. Entries in the map will have a pointer to the corresponding item in the linked list, so that whenever you access an item, you can move that item to the front of the linked list. That way the least recently accessed item is alway the tail item of the linked list.</span></p></div></div><div id="wallapatta_58" class="special"><div id="wallapatta_59" class="section"><div class="content"><p id="wallapatta_60" class="paragraph"><a id="wallapatta_281" class="link" href="https://www.geeksforgeeks.org/lru-cache-implementation/">Here</a><span id="wallapatta_282" class="text"> is a general explanation on how LRU caches are implemented.</span></p></div></div></div><div id="wallapatta_64" class="section"><div class="content"><p id="wallapatta_65" class="paragraph"><span id="wallapatta_284" class="text">You can find my LRU cache implementation on Ballerina </span><a id="wallapatta_285" class="link" href="https://github.com/chethiya/ballerina-cache">here</a><span id="wallapatta_286" class="text">.</span></p></div></div><div id="wallapatta_66" class="section"><div class="content"><p id="wallapatta_67" class="paragraph"><strong id="wallapatta_287" class="bold"><span id="wallapatta_288" class="text">Few important points regarding this specific implementation:</span></strong></p><ul id="wallapatta_68" class="list"><li id="wallapatta_69" class="list-item"><span id="wallapatta_70" class="block"><span id="wallapatta_289" class="text">It uses </span><code id="wallapatta_290" class="code"><span id="wallapatta_291" class="text">time:nanoTime()</span></code><span id="wallapatta_292" class="text"> not because we need nanosecond precision. In fact their is no guarantee that underlying hardware clock has nanosecond accuracy. I'm using it simply because </span><code id="wallapatta_293" class="code"><span id="wallapatta_294" class="text">time:currentTime():time</span></code><span id="wallapatta_295" class="text"> seems to be 10x slower than </span><code id="wallapatta_296" class="code"><span id="wallapatta_297" class="text">time:nanoTime()</span></code><span id="wallapatta_298" class="text">.</span></span></li></ul><div id="wallapatta_74" class="special"><div id="wallapatta_75" class="section"><div class="content"><p id="wallapatta_76" class="paragraph"><span id="wallapatta_305" class="text">By the way </span><code id="wallapatta_306" class="code"><span id="wallapatta_307" class="text">time:nanoTime()</span></code><span id="wallapatta_308" class="text"> does not return the current timestamp since epoch date. It's just a wrapper to </span><a id="wallapatta_309" class="link" href="https://docs.oracle.com/javase/8/docs/api/java/lang/System.html#nanoTime%2D%2D">System.nanoTime in java</a><span id="wallapatta_310" class="text"> which returns a time since some arbitrary reference time. It's safe to be used here as long as this program is not going to run 100 years continuously without a restart.</span></p></div></div></div><ul id="wallapatta_77" class="list"><li id="wallapatta_78" class="list-item"><span id="wallapatta_79" class="block"><span id="wallapatta_311" class="text">Right now Ballerina maps are thread-safe. So that synchronizations on map operations are redundant, as all LRU cache operations anyway need to be synchronized using </span><code id="wallapatta_312" class="code"><span id="wallapatta_313" class="text">lock</span></code><span id="wallapatta_314" class="text">.</span></span></li><li id="wallapatta_80" class="list-item"><span id="wallapatta_81" class="block"><span id="wallapatta_315" class="text">I initially implemented </span><code id="wallapatta_316" class="code"><span id="wallapatta_317" class="text">CacheItem</span></code><span id="wallapatta_318" class="text"> as a </span><a id="wallapatta_319" class="link" href="https://ballerina.io/learn/by-example/records.html">Ballerina Closed Record</a><span id="wallapatta_320" class="text">. But looking at Ballerina code, it looks like Records are implemented as Hashtables. If you look at caching implementation, attributes of </span><code id="wallapatta_321" class="code"><span id="wallapatta_322" class="text">CacheItem</span></code><span id="wallapatta_323" class="text">s are accessed all the time. So having to access these attributes using a Hashtable doesn't make much sense. Ideally it should be implemented using something like C struct. Therefore I changed </span><code id="wallapatta_324" class="code"><span id="wallapatta_325" class="text">CacheItem</span></code><span id="wallapatta_326" class="text"> implementation to use </span><a id="wallapatta_327" class="link" href="https://ballerina.io/learn/by-example/objects.html">Ballerina Objects</a><span id="wallapatta_328" class="text"> which is similar to Java classes.</span></span></li></ul></div></div><div id="wallapatta_82" class="section"><h3 class="heading"><span id="wallapatta_83" class="block"><span id="wallapatta_329" class="text">Performance</span></span></h3><div class="content"></div></div><div id="wallapatta_84" class="section"><div class="content"><p id="wallapatta_85" class="paragraph"><span id="wallapatta_330" class="text">Here's a comparison of the run time of the </span><code id="wallapatta_331" class="code"><span id="wallapatta_332" class="text">O(N)</span></code><span id="wallapatta_333" class="text"> implementation and the </span><code id="wallapatta_334" class="code"><span id="wallapatta_335" class="text">O(NM)</span></code><span id="wallapatta_336" class="text"> implementation in Ballerina Standard Library:</span></p></div></div><table id="wallapatta_86" class="table"><thead><tr><th colspan="1"><span id="wallapatta_87" class="block"><span id="wallapatta_337" class="text">Capacity </span><code id="wallapatta_338" class="code"><span id="wallapatta_339" class="text">(N)</span></code></span></th><th colspan="1"><span id="wallapatta_88" class="block"><code id="wallapatta_340" class="code"><span id="wallapatta_341" class="text">O(M)</span></code><span id="wallapatta_342" class="text"> Cache run-time (s)</span></span></th><th colspan="1"><span id="wallapatta_89" class="block"><code id="wallapatta_343" class="code"><span id="wallapatta_344" class="text">O(NM)</span></code><span id="wallapatta_345" class="text"> Cache run-time (s)</span></span></th></tr></thead><tbody><tr><td colspan="1"><span id="wallapatta_90" class="block"><span id="wallapatta_346" class="text">5</span></span></td><td colspan="1"><span id="wallapatta_91" class="block"><span id="wallapatta_347" class="text">11.876</span></span></td><td colspan="1"><span id="wallapatta_92" class="block"><span id="wallapatta_348" class="text">21.133</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_93" class="block"><span id="wallapatta_349" class="text">10</span></span></td><td colspan="1"><span id="wallapatta_94" class="block"><span id="wallapatta_350" class="text">15.760</span></span></td><td colspan="1"><span id="wallapatta_95" class="block"><span id="wallapatta_351" class="text">17.318</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_96" class="block"><span id="wallapatta_352" class="text">20</span></span></td><td colspan="1"><span id="wallapatta_97" class="block"><span id="wallapatta_353" class="text">13.688</span></span></td><td colspan="1"><span id="wallapatta_98" class="block"><span id="wallapatta_354" class="text">18.829</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_99" class="block"><span id="wallapatta_355" class="text">40</span></span></td><td colspan="1"><span id="wallapatta_100" class="block"><span id="wallapatta_356" class="text">16.180</span></span></td><td colspan="1"><span id="wallapatta_101" class="block"><span id="wallapatta_357" class="text">26.507</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_102" class="block"><span id="wallapatta_358" class="text">80</span></span></td><td colspan="1"><span id="wallapatta_103" class="block"><span id="wallapatta_359" class="text">17.271</span></span></td><td colspan="1"><span id="wallapatta_104" class="block"><span id="wallapatta_360" class="text">46.618</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_105" class="block"><span id="wallapatta_361" class="text">160</span></span></td><td colspan="1"><span id="wallapatta_106" class="block"><span id="wallapatta_362" class="text">17.502</span></span></td><td colspan="1"><span id="wallapatta_107" class="block"><span id="wallapatta_363" class="text">78.205</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_108" class="block"><span id="wallapatta_364" class="text">320</span></span></td><td colspan="1"><span id="wallapatta_109" class="block"><span id="wallapatta_365" class="text">17.851</span></span></td><td colspan="1"><span id="wallapatta_110" class="block"><span id="wallapatta_366" class="text">142.091</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_111" class="block"><span id="wallapatta_367" class="text">640</span></span></td><td colspan="1"><span id="wallapatta_112" class="block"><span id="wallapatta_368" class="text">18.129</span></span></td><td colspan="1"><span id="wallapatta_113" class="block"><span id="wallapatta_369" class="text">275.218</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_114" class="block"><span id="wallapatta_370" class="text">1000</span></span></td><td colspan="1"><span id="wallapatta_115" class="block"><span id="wallapatta_371" class="text">18.435</span></span></td><td colspan="1"><span id="wallapatta_116" class="block"><span id="wallapatta_372" class="text">423.605</span></span></td></tr></tbody></table><div id="wallapatta_117" class="section"><div class="content"><p id="wallapatta_118" class="paragraph"><em id="wallapatta_373" class="italics"><span id="wallapatta_374" class="text">You can find the program used to measure these numbers </span><a id="wallapatta_375" class="link" href="https://github.com/chethiya/ballerina-cache/blob/master/src/cache/main.bal">here</a></em></p></div></div><div id="wallapatta_119" class="special"><div id="wallapatta_120" class="section"><div class="content"><p id="wallapatta_121" class="paragraph"><em id="wallapatta_376" class="italics"><span id="wallapatta_377" class="text">Measured times are real </span><strong id="wallapatta_378" class="bold"><span id="wallapatta_379" class="text">elapsed time</span></strong><span id="wallapatta_380" class="text">. Not </span><strong id="wallapatta_381" class="bold"><span id="wallapatta_382" class="text">CPU time</span></strong><span id="wallapatta_383" class="text">, which I couldn't find a way to measure in Ballerina.</span></em></p></div></div></div><div id="wallapatta_122" class="section"><div class="content"><p id="wallapatta_123" class="paragraph"><span id="wallapatta_384" class="text">As you can see the run-time of the cache implementation in Ballerina Standard Library varies linearly as </span><code id="wallapatta_385" class="code"><span id="wallapatta_386" class="text">N</span></code><span id="wallapatta_387" class="text"> grows. But stays quite stable for smaller </span><code id="wallapatta_388" class="code"><span id="wallapatta_389" class="text">N</span></code><span id="wallapatta_390" class="text">, specially for values smaller than 20 or so. I guess this is probably due to CPU caching that takes place because of all operations occur in a smaller array when it computes the evicting entries.</span></p></div></div><div id="wallapatta_124" class="section"><div class="content"><p id="wallapatta_125" class="paragraph"><span id="wallapatta_391" class="text">It looks like there's a slight increase in time as </span><code id="wallapatta_392" class="code"><span id="wallapatta_393" class="text">N</span></code><span id="wallapatta_394" class="text"> increases for </span><code id="wallapatta_395" class="code"><span id="wallapatta_396" class="text">O(M)</span></code><span id="wallapatta_397" class="text"> implementation. To verify this I tried with few different </span><code id="wallapatta_398" class="code"><span id="wallapatta_399" class="text">N</span></code><span id="wallapatta_400" class="text"> and had following results:</span></p></div></div><table id="wallapatta_126" class="table"><thead><tr><th colspan="1"><span id="wallapatta_127" class="block"><span id="wallapatta_401" class="text">Capacity </span><code id="wallapatta_402" class="code"><span id="wallapatta_403" class="text">(N)</span></code></span></th><th colspan="1"><span id="wallapatta_128" class="block"><code id="wallapatta_404" class="code"><span id="wallapatta_405" class="text">O(M)</span></code><span id="wallapatta_406" class="text"> Cache run-time (s)</span></span></th></tr></thead><tbody><tr><td colspan="1"><span id="wallapatta_129" class="block"><span id="wallapatta_407" class="text">10</span><sup id="wallapatta_408" class="superScript"><span id="wallapatta_409" class="text">2</span></sup></span></td><td colspan="1"><span id="wallapatta_130" class="block"><span id="wallapatta_410" class="text">10.470</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_131" class="block"><span id="wallapatta_411" class="text">10</span><sup id="wallapatta_412" class="superScript"><span id="wallapatta_413" class="text">3</span></sup></span></td><td colspan="1"><span id="wallapatta_132" class="block"><span id="wallapatta_414" class="text">10.545</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_133" class="block"><span id="wallapatta_415" class="text">10</span><sup id="wallapatta_416" class="superScript"><span id="wallapatta_417" class="text">4</span></sup></span></td><td colspan="1"><span id="wallapatta_134" class="block"><span id="wallapatta_418" class="text">13.518</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_135" class="block"><span id="wallapatta_419" class="text">10</span><sup id="wallapatta_420" class="superScript"><span id="wallapatta_421" class="text">5</span></sup></span></td><td colspan="1"><span id="wallapatta_136" class="block"><span id="wallapatta_422" class="text">19.555</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_137" class="block"><span id="wallapatta_423" class="text">10</span><sup id="wallapatta_424" class="superScript"><span id="wallapatta_425" class="text">6</span></sup><span id="wallapatta_426" class="text"> (with default heap size)</span></span></td><td colspan="1"><span id="wallapatta_138" class="block"><span id="wallapatta_427" class="text">GC overhead limit exceeded</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_139" class="block"><span id="wallapatta_428" class="text">10</span><sup id="wallapatta_429" class="superScript"><span id="wallapatta_430" class="text">6</span></sup><span id="wallapatta_431" class="text"> (with 256MB)</span></span></td><td colspan="1"><span id="wallapatta_140" class="block"><span id="wallapatta_432" class="text">44.177</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_141" class="block"><span id="wallapatta_433" class="text">10</span><sup id="wallapatta_434" class="superScript"><span id="wallapatta_435" class="text">6</span></sup><span id="wallapatta_436" class="text"> (with 512MB)</span></span></td><td colspan="1"><span id="wallapatta_142" class="block"><span id="wallapatta_437" class="text">21.131</span></span></td></tr><tr><td colspan="1"><span id="wallapatta_143" class="block"><span id="wallapatta_438" class="text">10</span><sup id="wallapatta_439" class="superScript"><span id="wallapatta_440" class="text">6</span></sup><span id="wallapatta_441" class="text"> (with 1024MB)</span></span></td><td colspan="1"><span id="wallapatta_144" class="block"><span id="wallapatta_442" class="text">18.156</span></span></td></tr></tbody></table><div id="wallapatta_145" class="section"><div class="content"><p id="wallapatta_146" class="paragraph"><span id="wallapatta_443" class="text">For </span><code id="wallapatta_444" class="code"><span id="wallapatta_445" class="text">N</span></code><span id="wallapatta_446" class="text">=10</span><sup id="wallapatta_447" class="superScript"><span id="wallapatta_448" class="text">6</span></sup><span id="wallapatta_449" class="text"> with the default heap size, it crashes giving following error:</span></p></div></div><pre id="wallapatta_147" class="codeBlock"><code class="nohighlight">java.lang.OutOfMemoryError: GC overhead limit exceeded</code></pre><div id="wallapatta_151" class="section"><div class="content"><p id="wallapatta_152" class="paragraph"><span id="wallapatta_451" class="text">I increased the Java heap size and got rest of the results after the failed test. Here we observe that as heap size increases, the runt-time performance also becomes better. I think this is due to the additional computations that need to be done when trying to allocate space for objects within a small heap vs doing that in a larger heap.</span></p></div></div><div id="wallapatta_156" class="section"><div class="content"><p id="wallapatta_157" class="paragraph"><span id="wallapatta_456" class="text">So the slight increment in run-time as </span><code id="wallapatta_457" class="code"><span id="wallapatta_458" class="text">N</span></code><span id="wallapatta_459" class="text"> grows is due to additional memory allocations it has to do within the same heap size.</span></p></div></div><div id="wallapatta_158" class="section"><div class="content"><p id="wallapatta_159" class="paragraph"><span id="wallapatta_460" class="text">In my opinion it would have been nice if the compiler could handle 1 million records (60 bytes per each record) in a map within a 128MB heap. It's quite interesting to implement same Cache in Java and see whether it can handle 1 million items within 128MB. Anyway it's not a big issue whereas the language doesn't seem to worry about those aspects too much right now.</span></p></div></div><div id="wallapatta_163" class="section"><h3 class="heading"><span id="wallapatta_164" class="block"><span id="wallapatta_465" class="text">How to use</span></span></h3><div class="content"></div></div><div id="wallapatta_165" class="section"><div class="content"><p id="wallapatta_166" class="paragraph"><span id="wallapatta_466" class="text">You can find my implementaion as a module at </span><a id="wallapatta_467" class="link" href="https://central.ballerina.io/chethiya/cache">Ballerina Central</a><span id="wallapatta_468" class="text">.</span></p></div></div><div id="wallapatta_170" class="section"><div class="content"><p id="wallapatta_171" class="paragraph"><span id="wallapatta_470" class="text">You can pull it within your code by adding it as a dependency in your project by editing </span><code id="wallapatta_471" class="code"><span id="wallapatta_472" class="text">.toml</span></code><span id="wallapatta_473" class="text"> file. e.g.</span></p></div></div><pre id="wallapatta_172" class="codeBlock"><code class="nohighlight">[dependencies]
"chethiya/cache" = "0.2.1"</code></pre><div id="wallapatta_173" class="section"><div class="content"><p id="wallapatta_174" class="paragraph"><span id="wallapatta_474" class="text">Run following command to pull the module:</span></p></div></div><pre id="wallapatta_175" class="codeBlock"><code class="nohighlight">ballerina pull chethiya/cache</code></pre><div id="wallapatta_176" class="section"><div class="content"><p id="wallapatta_177" class="paragraph"><span id="wallapatta_475" class="text">Now you are ready to use the </span><code id="wallapatta_476" class="code"><span id="wallapatta_477" class="text">LRUCache</span></code><span id="wallapatta_478" class="text"> in your code. Simply import the module and create an instance of it in your </span><code id="wallapatta_479" class="code"><span id="wallapatta_480" class="text">.bal</span></code><span id="wallapatta_481" class="text"> file. e.g.</span></p></div></div><pre id="wallapatta_178" class="codeBlock"><code class="java"><span class="hljs-keyword">import</span> ballerina/http;
<span class="hljs-keyword">import</span> ballerina/log;
<span class="hljs-keyword">import</span> chethiya/cache;

<span class="hljs-comment">// Keep 1000 search results and expire 1 mininutes after setting</span>
<span class="hljs-comment">// search results.</span>
cache:LRUCache cachedResults = <span class="hljs-keyword">new</span>(<span class="hljs-number">1000</span>, <span class="hljs-number">60000</span>, <span class="hljs-keyword">false</span>);

service hello on <span class="hljs-keyword">new</span> http:Listener(<span class="hljs-number">9090</span>) {
  <span class="hljs-function">resource function <span class="hljs-title">sayHello</span><span class="hljs-params">(http:Caller caller, http:Request req)</span> </span>{
    <span class="hljs-keyword">var</span> query = getQuery(req);
    SearchResult? result = cachedResults.get(query);
    <span class="hljs-keyword">if</span> (<span class="hljs-function">result <span class="hljs-title">is</span> <span class="hljs-params">()</span>) </span>{
      result = computeResult(query);
      cachedResults.put(query, result);
    }
    <span class="hljs-keyword">var</span> res = caller-&gt;respond(result);
    <span class="hljs-keyword">if</span> (res is error) {
        log:printError(<span class="hljs-string">"Error sending response"</span>, result);
    }
  }
}</code></pre><div id="wallapatta_184" class="section"><div class="content"><p id="wallapatta_185" class="paragraph"><span id="wallapatta_484" class="text">If you have any issue regarding this implementation, please report those </span><a id="wallapatta_485" class="link" href="https://github.com/chethiya/ballerina-cache/issues">here</a><span id="wallapatta_486" class="text">.</span></p></div></div></div>
     </div>
     <div class="wallapatta-sidebar three columns">
      <div id="wallapatta_5" class="sidenote"><div id="wallapatta_6" class="section"><div class="content"><p id="wallapatta_7" class="paragraph"><span id="wallapatta_191" class="text">LRU means </span><em id="wallapatta_192" class="italics"><span id="wallapatta_193" class="text">Least Recently Used</span></em><span id="wallapatta_194" class="text">. That's the eviction policy, the policy that is used to remove items from cache to make room for new items.</span></p></div></div></div><div id="wallapatta_13" class="sidenote"><div id="wallapatta_14" class="section"><div class="content"><p id="wallapatta_15" class="paragraph"><span id="wallapatta_197" class="text">If outcomes are not time sensitive, then caching most frequently used outcomes might be a better choice.</span></p></div></div></div><div id="wallapatta_18" class="sidenote"><div id="wallapatta_19" class="section"><div class="content"><p id="wallapatta_20" class="paragraph"><span id="wallapatta_201" class="text">You can find some examples </span><a id="wallapatta_202" class="link" href="https://ballerina.io/learn/by-example/">here</a><span id="wallapatta_203" class="text">. See how easy it is to create a simple service with it </span><a id="wallapatta_204" class="link" href="https://ballerina.io/learn/by-example/hello-world-service.html">here</a><span id="wallapatta_205" class="text">. It'll run in a pre allocated thread pool and you don't have to worry about settings those up.</span></p></div></div></div><div id="wallapatta_30" class="sidenote"><div id="wallapatta_31" class="section"><div class="content"><p id="wallapatta_32" class="paragraph"><span id="wallapatta_226" class="text">So this implementation doesn't scale well when cache size grows. i.e. higher the number of outcomes you want to cache, slower it'll perform.</span></p></div></div></div><div id="wallapatta_51" class="sidenote"><div id="wallapatta_52" class="section"><div class="content"><p id="wallapatta_53" class="paragraph"><span id="wallapatta_276" class="text">This is the total time taken for  eviction, which is the dominant part of total time.</span></p></div></div></div><div id="wallapatta_61" class="sidenote"><div id="wallapatta_62" class="section"><div class="content"><p id="wallapatta_63" class="paragraph"><span id="wallapatta_283" class="text">GeeksforGeeks is a really good resource if you want to revise/learn algorithms and data structures. Also it's a quite good resource if you are preparing for coding interviews.</span></p></div></div></div><div id="wallapatta_71" class="sidenote"><div id="wallapatta_72" class="section"><div class="content"><p id="wallapatta_73" class="paragraph"><code id="wallapatta_299" class="code"><span id="wallapatta_300" class="text">time:currentTime()</span></code><span id="wallapatta_301" class="text"> creates a record by allocating memory and right now it's a hashtable. </span><code id="wallapatta_302" class="code"><span id="wallapatta_303" class="text">time:nanoTime()</span></code><span id="wallapatta_304" class="text"> just returns the integer so it is much faster.</span></p></div></div></div><div id="wallapatta_148" class="sidenote"><div id="wallapatta_149" class="section"><div class="content"><p id="wallapatta_150" class="paragraph"><span id="wallapatta_450" class="text">I think the default heap size of Ballerina is 128MB</span></p></div></div></div><div id="wallapatta_153" class="sidenote"><div id="wallapatta_154" class="section"><div class="content"><p id="wallapatta_155" class="paragraph"><span id="wallapatta_452" class="text">Do </span><code id="wallapatta_453" class="code"><span id="wallapatta_454" class="text">export JAVA_OPTS="-Xmx512m"</span></code><span id="wallapatta_455" class="text"> to increase heap</span></p></div></div></div><div id="wallapatta_160" class="sidenote"><div id="wallapatta_161" class="section"><div class="content"><p id="wallapatta_162" class="paragraph"><span id="wallapatta_461" class="text">A </span><code id="wallapatta_462" class="code"><span id="wallapatta_463" class="text">CacheItem</span></code><span id="wallapatta_464" class="text"> takes ~60 bytes assuming strings have 4 bytes for each character.</span></p></div></div></div><div id="wallapatta_167" class="sidenote"><div id="wallapatta_168" class="section"><div class="content"><p id="wallapatta_169" class="paragraph"><span id="wallapatta_469" class="text">It's a public package repository for Ballerina.</span></p></div></div></div><div id="wallapatta_179" class="sidenote"><div id="wallapatta_180" class="section"><div class="content"><p id="wallapatta_181" class="paragraph"><span id="wallapatta_482" class="text">Note the initializer is little different in this compared to Stdlib Cache.</span></p></div></div><div id="wallapatta_182" class="section"><div class="content"><p id="wallapatta_183" class="paragraph"><span id="wallapatta_483" class="text">Note how it can expire entries based on time, only considering SET time irrespective of SET times.</span></p></div></div></div>
     </div>
     <div style="display:none;">
      <div class='wallapatta-code'>### LRU Cache

Caching is a quite useful concept, specially if you have relatively heavy computations that might not need to be done every single time. Among many different types of caches, LRU caches are a quite popular choice. The basic idea is to retain most recently used --n-- outcomes that can fit inside the allocated memory for the cache. So those cached outcomes can be used to serve subsequent queries without recomputing those outcomes. This is quite a good fit for most practical use cases when:

>>>
 LRU means --Least Recently Used--. That's the eviction policy, the policy that is used to remove items from cache to make room for new items.

* same query is likely to occur in a reasonably small time window,
* computed outcomes for queries are time sensitive. i.e. outcomes depends on the time it's computed.

>>>
 If outcomes are not time sensitive, then caching most frequently used outcomes might be a better choice.

I was recently asked to implement an LRU cache during a coding interview and I was quite happy that interviewers do ask non hypothetical questions too. Anyway this article is not about interviews. It's an LRU cache implementation for a new programming language called <<https://github.com/ballerina-platform/ballerina-lang(Ballerina)>>.

>>>
 You can find some examples <<https://ballerina.io/learn/by-example/(here)>>. See how easy it is to create a simple service with it <<https://ballerina.io/learn/by-example/hello-world-service.html(here)>>. It'll run in a pre allocated thread pool and you don't have to worry about settings those up.



### Current cache implementation of Ballerina in its Standard Library

I wanted to give Ballerina a try for a long time, but I didn't get a chance. Finally I got some time to dig into their codebase and try it out. I was quite impressed by the fact that they have included an <<https://ballerina.io/learn/api-docs/ballerina/cache/index.html(LRU cache implementation in their standard library)>> and it's shipped together with the compiler. It looks like that caching implementation is consumed in few other places inside other standard libraries, like the <<https://ballerina.io/learn/api-docs/ballerina/http/index.html(HTTP module)>>.

I looked at their caching implementation and found these issues with it:

* Its runtime complexity is not the optimal for a LRU Cache. If size of the buffer is ``N`` and if there are ``M`` cache queries (assuming GET/SET ratio is constant) then its run time complexity is ``O(NM)``. The LRU Cache can be implemented to run in ``O(M)``.

>>>
 So this implementation doesn't scale well when cache size grows. i.e. higher the number of outcomes you want to cache, slower it'll perform.

* It has a parameter called --eviction factor-- ``f``, and what it does is once the cache is full it removes ``N*f`` least recently used items. This behavior doesn't utilize allocated memory for the cache in 100%, but average utilization is something like ``(1-f)*100 + (f*100/2)`` percent.

* It compares between all timestamps of the items to compute the least recently used ``N*f`` items to be evicted. But this depends on the precision of the timestamp. It currently stores timestamps in milliseconds. So if all items in the cache are accessed in the same millisecond, then above computation would be inaccurate.

* It has a separate task running every 5 seconds to cleanup expired cache entries. The way it's implemented this also introduces a shared lock between all cache instances. Altogether this cleanup processes is unnecessary work as this check can be done only during the GET operation. Also it doesn't make much sense to do this regular cleanup work for saving memory. Usually LRU caches are expected to be full all the time. If not consumer should consider reducing the amount of memory (i.e. ``N``) allocated for the cache.

* It seems to be written in a non thread-safe manner. See <<https://github.com/ballerina-platform/ballerina-lang/issues/19187#issuecomment-544975764(here)>> the comment I made in an issue.

+++
 **Here how the current implementation takes ``O(NM)`` time:**

 Once cache is reached to it's capacity it computes ``Nf`` least recently used entries in cache. That's done in ``O(N*Nf)``, using 2 loops. This could have been done much faster using a heap, but that approach won't help us much to get the overall time complexity down to ``O(M)``

 Out of ``M`` cache access, if we assume the ratio between GET/SET operations is constant, then we can assume the above eviction is going to happen ``O(M/(N*f))`` times. Note that the "Big O" eliminates that constant factor.

 So total run time complexity is ``O((M/Nf) * (N*Nf))=O(MN)``.

 >>>
  This is the total time taken for  eviction, which is the dominant part of total time.


### How to do it in ``O(M)``

We should be able to implement LRU cache so that its time complexity is independent from the cache size. Key idea to do so is to use a separate linked list to store entries sorted by their access time. Entries in the map will have a pointer to the corresponding item in the linked list, so that whenever you access an item, you can move that item to the front of the linked list. That way the least recently accessed item is alway the tail item of the linked list.

+++
 <<https://www.geeksforgeeks.org/lru-cache-implementation/(Here)>> is a general explanation on how LRU caches are implemented.

>>>
 GeeksforGeeks is a really good resource if you want to revise/learn algorithms and data structures. Also it's a quite good resource if you are preparing for coding interviews.

You can find my LRU cache implementation on Ballerina <<https://github.com/chethiya/ballerina-cache(here)>>.

**Few important points regarding this specific implementation:**

 * It uses ``time:nanoTime()`` not because we need nanosecond precision. In fact their is no guarantee that underlying hardware clock has nanosecond accuracy. I'm using it simply because ``time:currentTime():time`` seems to be 10x slower than ``time:nanoTime()``.

 >>>
  ``time:currentTime()`` creates a record by allocating memory and right now it's a hashtable. ``time:nanoTime()`` just returns the integer so it is much faster.

 +++
  By the way ``time:nanoTime()`` does not return the current timestamp since epoch date. It's just a wrapper to <<https://docs.oracle.com/javase/8/docs/api/java/lang/System.html#nanoTime%2D%2D(System.nanoTime in java)>> which returns a time since some arbitrary reference time. It's safe to be used here as long as this program is not going to run 100 years continuously without a restart.

 * Right now Ballerina maps are thread-safe. So that synchronizations on map operations are redundant, as all LRU cache operations anyway need to be synchronized using ``lock``.

 * I initially implemented ``CacheItem`` as a <<https://ballerina.io/learn/by-example/records.html(Ballerina Closed Record)>>. But looking at Ballerina code, it looks like Records are implemented as Hashtables. If you look at caching implementation, attributes of ``CacheItem``s are accessed all the time. So having to access these attributes using a Hashtable doesn't make much sense. Ideally it should be implemented using something like C struct. Therefore I changed ``CacheItem`` implementation to use <<https://ballerina.io/learn/by-example/objects.html(Ballerina Objects)>> which is similar to Java classes.

### Performance

Here's a comparison of the run time of the ``O(N)`` implementation and the ``O(NM)`` implementation in Ballerina Standard Library:

|||
 Capacity ``(N)`` | ``O(M)`` Cache run-time (s) | ``O(NM)`` Cache run-time (s)
 ===
 5 | 11.876 | 21.133
 10 | 15.760 | 17.318
 20 | 13.688 | 18.829
 40 | 16.180 | 26.507
 80 | 17.271 | 46.618
 160 | 17.502 | 78.205
 320 | 17.851 | 142.091
 640 | 18.129 | 275.218
 1000 | 18.435 | 423.605

--You can find the program used to measure these numbers <<https://github.com/chethiya/ballerina-cache/blob/master/src/cache/main.bal(here)>>--

+++
 --Measured times are real **elapsed time**. Not **CPU time**, which I couldn't find a way to measure in Ballerina.--

As you can see the run-time of the cache implementation in Ballerina Standard Library varies linearly as ``N`` grows. But stays quite stable for smaller ``N``, specially for values smaller than 20 or so. I guess this is probably due to CPU caching that takes place because of all operations occur in a smaller array when it computes the evicting entries.

It looks like there's a slight increase in time as ``N`` increases for ``O(M)`` implementation. To verify this I tried with few different ``N`` and had following results:

|||
 Capacity ``(N)`` | ``O(M)`` Cache run-time (s)
 ===
 10^^2^^ | 10.470
 10^^3^^ | 10.545
 10^^4^^ | 13.518
 10^^5^^ | 19.555
 10^^6^^ (with default heap size) | GC overhead limit exceeded
 10^^6^^ (with 256MB) | 44.177
 10^^6^^ (with 512MB) | 21.131
 10^^6^^ (with 1024MB) | 18.156

For ``N``=10^^6^^ with the default heap size, it crashes giving following error:

```
 java.lang.OutOfMemoryError: GC overhead limit exceeded

>>>
 I think the default heap size of Ballerina is 128MB

I increased the Java heap size and got rest of the results after the failed test. Here we observe that as heap size increases, the runt-time performance also becomes better. I think this is due to the additional computations that need to be done when trying to allocate space for objects within a small heap vs doing that in a larger heap.

>>>
 Do ``export JAVA_OPTS="-Xmx512m"`` to increase heap

So the slight increment in run-time as ``N`` grows is due to additional memory allocations it has to do within the same heap size.

In my opinion it would have been nice if the compiler could handle 1 million records (60 bytes per each record) in a map within a 128MB heap. It's quite interesting to implement same Cache in Java and see whether it can handle 1 million items within 128MB. Anyway it's not a big issue whereas the language doesn't seem to worry about those aspects too much right now.

>>>
 A ``CacheItem`` takes ~60 bytes assuming strings have 4 bytes for each character.

### How to use

You can find my implementaion as a module at <<https://central.ballerina.io/chethiya/cache(Ballerina Central)>>.

>>>
 It's a public package repository for Ballerina.

You can pull it within your code by adding it as a dependency in your project by editing ``.toml`` file. e.g.

```
 [dependencies]
 "chethiya/cache" = "0.2.1"

Run following command to pull the module:

```
 ballerina pull chethiya/cache

Now you are ready to use the ``LRUCache`` in your code. Simply import the module and create an instance of it in your ``.bal`` file. e.g.

```java
 import ballerina/http;
 import ballerina/log;
 import chethiya/cache;

 // Keep 1000 search results and expire 1 mininutes after setting
 // search results.
 cache:LRUCache cachedResults = new(1000, 60000, false);

 service hello on new http:Listener(9090) {
   resource function sayHello(http:Caller caller, http:Request req) {
     var query = getQuery(req);
     SearchResult? result = cachedResults.get(query);
     if (result is ()) {
       result = computeResult(query);
       cachedResults.put(query, result);
     }
     var res = caller->respond(result);
     if (res is error) {
         log:printError("Error sending response", result);
     }
   }
 }

>>>
 Note the initializer is little different in this compared to Stdlib Cache.

 Note how it can expire entries based on time, only considering SET time irrespective of SET times.

If you have any issue regarding this implementation, please report those <<https://github.com/chethiya/ballerina-cache/issues(here)>>.

</div>
     </div>
    </div>
   </div>
  </div>
  <script src="lib/highlightjs/highlight.pack.js">
  </script>
  <script src="lib/weya/weya.js">
  </script>
  <script src="lib/weya/base.js">
  </script>
  <script src="lib/mod/mod.js">
  </script>
  <script src="js/static.js?v=9">
  </script>
  <script src="js/parser.js?v=9">
  </script>
  <script src="js/reader.js?v=9">
  </script>
  <script src="js/nodes.js?v=9">
  </script>
  <script src="js/render.js?v=9">
  </script>
 </body>
</html>
